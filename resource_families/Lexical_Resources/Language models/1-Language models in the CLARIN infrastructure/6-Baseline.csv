"Corpus";"Corpus_URL";"Size";"Annotation";"Licence";"Language";"Description";"Buttons";"Buttons_URL";"Publication";"Publication_URL";"Note"
"CERED baseline models";"http://hdl.handle.net/11234/1-3266";;"Baseline";"CC BY-NC-SA 4.0";"Czech";"These models are trained on <a href=""http://hdl.handle.net/11234/1-3265"">CERED</a>, a dataset created by distant supervision on Czech Wikipedia and Wikidata, and recognize a subset of Wikidata relations.#SEPThe model is available for download from the LINDAT repository.";"Download";"http://hdl.handle.net/11234/1-3266";;;
"LVBERT - Latvian BERT";"http://hdl.handle.net/20.500.12574/43";;"Baseline";"GNU GPL3";"Latvian";"This model is trained on the original implementation of <a href=""https://github.com/google-research/bert"">BERT</a> on the <a href=""https://www.tensorflow.org/"">TensorFlow</a> machine-learning platform with the whole-word masking and the next sentence prediction objectives. This uses the BERT configuration with 12 layers, 768 hidden units, 12 heads, 128 sequence length, 128 mini-batch size and a 32,000 token vocabulary.#SEPTHe model is available for download from the CLARIN-LV repository.";"Download";"http://hdl.handle.net/20.500.12574/43";"Znotinš and Barzdinš (2020)";"https://doi.org/10.3233/FAIA200610";
"LitLat BERT";"http://hdl.handle.net/20.500.11821/42";;"Baseline";"PUB CLARIN-LT";"Lithuanian, Latvian, English";"This <a href=""https://github.com/google-research/bert"">BERT</a>-like model represents tokens as contextually dependent word embeddings, used for various NLP classification tasks by fine-tuning the model end-to-end. The corpora used for training the model have 4.07 billion tokens in total, of which 2.32 billion are English, 1.21 billion are Lithuanian and 0.53 billion are Latvian.#SEPThe model is available for download from the CLARIN-LT repository.
";"Download";"http://hdl.handle.net/20.500.11821/42";;;
"BERTimbau - Portuguese BERT-Base language model";"https://hdl.handle.net/21.11129/0000-000E-6726-4";;"Baseline";"Under negotiation";"Portuguese";"This is a <a href=""https://github.com/google-research/bert"">BERT</a> model, trained on <a href=""https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC#Current_version"">BrWaC</a> (Brazilian Web as Corpus), a large Portuguese corpus, for 1,000,000 steps, using whole-word mask.#SEPThe model is available for download from the PORTULAN repository.";"Download";"https://github.com/neuralmind-ai/portuguese-bert/";;;
"BERTimbau - Portuguese BERT-Large language model";"https://hdl.handle.net/21.11129/0000-000E-6725-5";;"Baseline";"Under negotiation";"Portuguese";"This is a <a href=""https://github.com/google-research/bert"">BERT</a> model, trained on <a href=""https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC#Current_version"">BrWaC</a> (Brazilian Web as Corpus), a large Portuguese corpus, for 1,000,000 steps, using whole-word mask.#SEPThe model is available for download from the PORTULAN repository.";"Download";"https://github.com/neuralmind-ai/portuguese-bert/";;;
"Portuguese RoBERTa language model";"https://hdl.handle.net/21.11129/0000-000E-631E-2";;"Baseline";"CC-BY";"Portuguese";"This is a pre-trained <a href=""https://huggingface.co/docs/transformers/model_doc/roberta"">roBERTa</a> model in Portuguese, with 6 layers and 12 attention-heads, totaling 68M parameters. Pre-training was done on 10 million Portuguese sentences and 10 million English sentences from the <a href=""https://oscar-corpus.com/"">OSCAR corpus</a>.#SEPThe model is available for download from the PORUTLAN repository.";"Download";"https://hdl.handle.net/21.11129/0000-000E-631E-2";;;
"Dataset and baseline model of moderated content FRENK-MMC-RTV 1.0";"http://hdl.handle.net/11356/1201";;"Baseline";"CC BY-SA 4.0";"Slovenian";"FRENK-MMC-RTV is a dataset of moderated newspaper comments from the website <a href=""https://www.rtvslo.si/"">rtvslo.si</a> with metadata on the time of publishing, user identifier, thread identifier and whether the comment was deleted by the moderators or not. The full text of each comment is encrypted via a character-replacement method so that the comments are not readable by humans. Basic punctuation is not encrypted in order to enable tokenization. The main use of this dataset are experiments on automating comment moderation. For real-world usage, a fastText classification model trained on non-encrypted data is made available as well.#SEPThe model is available for download from the CLARIN.SI repository.";"Download";"http://hdl.handle.net/11356/1201";"Ljubešić et al. (2018)";"https://aclanthology.org/W18-5116/";
"ccGigafida ARPA language model 1.0";"http://hdl.handle.net/11356/1119";;"Baseline";"CC BY 4.0";"Slovenian";"This model was created from the <a href=""http://hdl.handle.net/11356/1035"">ccGigafida written corpus of Slovenian</a> using the <a href=""https://github.com/kpu/kenlm"">KenLM algorithm</a> in the <a href=""http://www2.statmt.org/moses/"">Moses machine translation framework</a>. It is a general language model of contemporary standard Slovenian language that can be used as a language model in statistical machine translation systems.#SEPThe model is available for download from the CLARIN.SI repository.";"Download";"http://hdl.handle.net/11356/1119";;;
