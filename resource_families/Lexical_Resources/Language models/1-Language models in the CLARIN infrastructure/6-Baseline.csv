"Corpus";"Corpus_URL";"Size";"Annotation";"Licence";"Licence_URL";"Language";"Description";"Family";"Buttons";"Buttons_URL";"Publication";"Publication_URL";"Note"
"CERED baseline models";"http://hdl.handle.net/11234/1-3266";;"Baseline";"CC BY-NC-SA 4.0";;"Czech";"These models are trained on <a href=""http://hdl.handle.net/11234/1-3265"">CERED</a>, a dataset created by distant supervision on Czech Wikipedia and Wikidata, and recognize a subset of Wikidata relations.#SEPThe model is available for download from the LINDAT repository.";"Language models";"Download";"http://hdl.handle.net/11234/1-3266";;;
"LVBERT - Latvian BERT";"http://hdl.handle.net/20.500.12574/43";;"Baseline";"GNU GPL3";;"Latvian";"This model is trained on the original implementation of <a href=""https://github.com/google-research/bert"">BERT</a> on the <a href=""https://www.tensorflow.org/"">TensorFlow</a> machine-learning platform with the whole-word masking and the next sentence prediction objectives. This uses the BERT configuration with 12 layers, 768 hidden units, 12 heads, 128 sequence length, 128 mini-batch size and a 32,000 token vocabulary.#SEPTHe model is available for download from the CLARIN-LV repository.";"Language models";"Download";"http://hdl.handle.net/20.500.12574/43";"Znotinš and Barzdinš (2020)";"https://doi.org/10.3233/FAIA200610";
"LitLat BERT";"http://hdl.handle.net/20.500.11821/42";;"Baseline";"PUB CLARIN-LT";;"Lithuanian, Latvian, English";"This <a href=""https://github.com/google-research/bert"">BERT</a>-like model represents tokens as contextually dependent word embeddings, used for various NLP classification tasks by fine-tuning the model end-to-end. The corpora used for training the model have 4.07 billion tokens in total, of which 2.32 billion are English, 1.21 billion are Lithuanian and 0.53 billion are Latvian.#SEPThe model is available for download from the CLARIN-LT repository.
";"Language models";"Download";"http://hdl.handle.net/20.500.11821/42";;;
"Albertina PT-BR";"https://hdl.handle.net/21.11129/0000-000F-FF43-7 ";;"Baseline";"MIT";;"Portuguese";"This model is an encoder of the BERT family and is based on the neural architecture Transformer and developed over the <a href=""https://huggingface.co/docs/transformers/model_doc/deberta"">DeBERTa</a> model. This model is for American Portuguese spoken in Brazil, is trained on the <a href=""https://huggingface.co/datasets/brwac"">brWaC</a> dataset, and is a larger version of the <a href=""https://hdl.handle.net/21.11129/0000-000F-FF45-5 "">Albertina PT-BR</a> base model.#SEPThis model is available for download through Hugging Face.";"Language models";"Download";"https://huggingface.co/PORTULAN/albertina-ptbr";;;
"Albertina PT-BR base";"https://hdl.handle.net/21.11129/0000-000F-FF45-5";;"Baseline";"MIT";;"Portuguese";"This model is for Portuguese spoken in Brazil. It is based on the Transformer neural architecture and is developed over the <a href=""https://huggingface.co/docs/transformers/model_doc/deberta"">DeBERTa model</a>. ";"Language models";"Download";"https://huggingface.co/PORTULAN/albertina-ptbr-base";;;
"Albertina PT-BR No-brWaC";"https://hdl.handle.net/21.11129/0000-000F-FF46-4 ";;"Baseline";"MIT";;"Portuguese";"This is a model for Portuguese spoken in Brazil trained on adta sets othan than brWaC. It is I developed over the <a href=""https://huggingface.co/docs/transformers/model_doc/deberta"">DeBERTa model</a>.  #SEPThe model is available for download from Hugging Face.";"Language models";"Download";"https://huggingface.co/PORTULAN/albertina-ptbr-nobrwac";;;
"Albertina PT-PT";"https://hdl.handle.net/21.11129/0000-000F-FF42-8";;"Baseline";"MIT";;"Portuguese";"This model is an encoder of the BERT family and is based on the neural architecture Transformer and developed over the <a href=""https://huggingface.co/docs/transformers/model_doc/deberta"">DeBERTa</a> model. This model is for European Portuguese and is trained on the <a href=""https://huggingface.co/datasets/brwac"">brWaC</a> dataset, and is a larger version of the <a href=""https://hdl.handle.net/21.11129/0000-000F-FF45-6"">Albertina PT-PT</a> base model.#SEPThis model is available for download through Hugging Face.";"Language models";"Download";"https://huggingface.co/PORTULAN/albertina-ptpt";;;
"Albertina PT-PT base";"https://hdl.handle.net/21.11129/0000-000F-FF44-6";;"Baseline";"MIT";;"Portuguese";"This model is for European. It is based on the Transformer neural architecture and is developed over the <a href=""https://huggingface.co/docs/transformers/model_doc/deberta"">DeBERTa model</a>.#SEPThis model is available for download through Hugging Face.";"Language models";"Download";"https://huggingface.co/PORTULAN/albertina-ptpt-base";;;
"Gervásio PT-BR base";"https://hdl.handle.net/21.11129/0000-000F-FF48-2 ";;"Baseline";"MIT";;"Portuguese";"This model, which is for Portuguese spoken in Brazil, is a decoder of the GPT family that is based on the neural architecture Transformer and developed over the <a href=""https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling"">Pythia model</a>.#SEPThe model is available for download from Hugging Face.";"Language models";"Download";"https://hdl.handle.net/21.11129/0000-000F-FF48-2 ";;;
"Gervásio PT-PT base";"https://hdl.handle.net/21.11129/0000-000F-FF47-3";;"Baseline";"MIT";;"Portuguese";"This model, which is for European Portuguese, is a decoder of the GPT family that is based on the neural architecture Transformer and developed over the <a href=""https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling"">Pythia model</a>.#SEPThe model is available for download from Hugging Face.";"Language models";"Download";"https://huggingface.co/PORTULAN/gervasio-ptpt-base";;;
"BERTimbau - Portuguese BERT-Base language model";"https://hdl.handle.net/21.11129/0000-000E-6726-4";;"Baseline";"Under negotiation";;"Portuguese";"This is a <a href=""https://github.com/google-research/bert"">BERT</a> model, trained on <a href=""https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC#Current_version"">BrWaC</a> (Brazilian Web as Corpus), a large Portuguese corpus, for 1,000,000 steps, using whole-word mask.#SEPThe model is available for download from the PORTULAN repository.";"Language models";"Download";"https://huggingface.co/PORTULAN/gervasio-ptpt";;;
"BERTimbau - Portuguese BERT-Large language model";"https://hdl.handle.net/21.11129/0000-000E-6725-5";;"Baseline";"Under negotiation";;"Portuguese";"This is a <a href=""https://github.com/google-research/bert"">BERT</a> model, trained on <a href=""https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC#Current_version"">BrWaC</a> (Brazilian Web as Corpus), a large Portuguese corpus, for 1,000,000 steps, using whole-word mask.#SEPThe model is available for download from the PORTULAN repository.";"Language models";"Download";"https://github.com/neuralmind-ai/portuguese-bert/";;;
"Portuguese RoBERTa language model";"https://hdl.handle.net/21.11129/0000-000E-631E-2";;"Baseline";"CC-BY";;"Portuguese";"This is a pre-trained <a href=""https://huggingface.co/docs/transformers/model_doc/roberta"">roBERTa</a> model in Portuguese, with 6 layers and 12 attention-heads, totaling 68M parameters. Pre-training was done on 10 million Portuguese sentences and 10 million English sentences from the <a href=""https://oscar-corpus.com/"">OSCAR corpus</a>.#SEPThe model is available for download from the PORUTLAN repository.";"Language models";"Download";"https://hdl.handle.net/21.11129/0000-000E-631E-2";;;
"Dataset and baseline model of moderated content FRENK-MMC-RTV 1.0";"http://hdl.handle.net/11356/1201";;"Baseline";"CC BY-SA 4.0";;"Slovenian";"FRENK-MMC-RTV is a dataset of moderated newspaper comments from the website <a href=""https://www.rtvslo.si/"">rtvslo.si</a> with metadata on the time of publishing, user identifier, thread identifier and whether the comment was deleted by the moderators or not. The full text of each comment is encrypted via a character-replacement method so that the comments are not readable by humans. Basic punctuation is not encrypted in order to enable tokenization. The main use of this dataset are experiments on automating comment moderation. For real-world usage, a fastText classification model trained on non-encrypted data is made available as well.#SEPThe model is available for download from the CLARIN.SI repository.";"Language models";"Download";"http://hdl.handle.net/11356/1201";"Ljubešić et al. (2018)";"https://aclanthology.org/W18-5116/";
"ccGigafida ARPA language model 1.0";"http://hdl.handle.net/11356/1119";;"Baseline";"CC BY 4.0";;"Slovenian";"This model was created from the <a href=""http://hdl.handle.net/11356/1035"">ccGigafida written corpus of Slovenian</a> using the <a href=""https://github.com/kpu/kenlm"">KenLM algorithm</a> in the <a href=""http://www2.statmt.org/moses/"">Moses machine translation framework</a>. It is a general language model of contemporary standard Slovenian language that can be used as a language model in statistical machine translation systems.#SEPThe model is available for download from the CLARIN.SI repository.";"Language models";"Download";"http://hdl.handle.net/11356/1119";;;
