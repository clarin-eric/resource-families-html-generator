"Corpus";"Corpus_URL";"Size";"Annotation";"Licence";"Language";"Description";"Buttons";"Buttons_URL";"Publication";"Publication_URL";"Note"
"CroSloEngual BERT 1.1";"http://hdl.handle.net/11356/1330";;"word embeddings";"CC BY-SA 4.0";"Croatian, English, Slovenian";"Trilingual BERT (Bidirectional Encoder Representations from Transformers) model, trained on Croatian, Slovenian, and English data. State of the art tool representing words/tokens as contextually dependent word embeddings, used for various NLP classification tasks by finetuning the model end-to-end. CroSloEngual BERT are neural network weights and configuration files in pytorch format (i.e. to be used with pytorch library).#SEPThe model is available for download from the CLARIN.SI repository.";"Download";"http://hdl.handle.net/11356/1330";"Ulčar and Robnik-Šikonja (2020)";"https://arxiv.org/abs/2006.07890";
"ELMo embeddings models for seven languages";"http://hdl.handle.net/11356/1277";;"word embeddings";"Apache License 2.0";"Croatian, Estonian, Finnish, Latvian, Lithuanian, Slovenian, Swedish";"This model is used to produce contextual word embeddings. It is trained on large monolingual corpora for 7 languages. Each language's model was trained for approximately 10 epochs. Corpora sizes used in training range from over 270 M tokens in Latvian to almost 2 B tokens in Croatian. About 1 million most common tokens were provided as vocabulary during the training for each language model. The model can also infer OOV words, since the neural network input is on the character level.#SEPThe model is available for download from the CLARIN.SI repository.";"Download";"http://hdl.handle.net/11356/1277";;;
"LX-DSemVectors";"https://hdl.handle.net/21.11129/0000-000B-D38A-B";;"Word embeddings";"CC-BY";"Portuguese";"This model represents tokens as contextual  word embeddings for Portuguese. It was trained on a corpus of 2 billion tokens and achieved state-of-the-art results on multiple lexical semantic tasks.#SEPThe model is available for download from the PORTULAN repository.";"Download";"https://hdl.handle.net/21.11129/0000-000B-D38A-B";;;
"Slovenian RoBERTa contextual embeddings model: SloBERTa 2.0";"http://hdl.handle.net/11356/1397";;"word embeddings";"CC BY-SA 4.0";"Slovenian";"The monolingual Slovene RoBERTa (A Robustly Optimized Bidirectional Encoder Representations from Transformers) model is a state-of-the-art model representing words/tokens as contextually dependent word embeddings, used for various NLP tasks. Word embeddings can be extracted for every word occurrence and then used in training a model for an end task, but typically the whole RoBERTa model is fine-tuned end-to-end.";"Download";;;;
"Word Embeddings trained on English Wikipedia";"https://spraakbanken.gu.se/en/resources/wikipedia-embeddings";;"word embeddings";"CC BY 4.0";"Swedish";"This is a set of contextual word embeddings.#SEPThe models are available for download from the Swedish Language Bank.";"Download";"https://spraakbanken.gu.se/en/resources/wikipedia-embeddings";;;
