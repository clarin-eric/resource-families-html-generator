Corpus;Corpus_URL;Size;Annotation;Licence;Licence_URL;Language;Description;Family;Buttons;Buttons_URL;Publication;Publication_URL;Note
CroSloEngual BERT 1.1;http://hdl.handle.net/11356/1330;;word embeddings;CC BY-SA 4.0;;Croatian, English, Slovenian;Trilingual BERT (Bidirectional Encoder Representations from Transformers) model, trained on Croatian, Slovenian, and English data. State of the art tool representing words/tokens as contextually dependent word embeddings, used for various NLP classification tasks by finetuning the model end-to-end. CroSloEngual BERT are neural network weights and configuration files in pytorch format (i.e. to be used with pytorch library).#SEPThe model is available for download from the CLARIN.SI repository.;Language models;Download;http://hdl.handle.net/11356/1330;Ulčar and Robnik-Šikonja (2020);https://arxiv.org/abs/2006.07890;
ELMo embeddings models for seven languages;http://hdl.handle.net/11356/1277;;word embeddings;Apache License 2.0;;Croatian, Estonian, Finnish, Latvian, Lithuanian, Slovenian, Swedish;This model is used to produce contextual word embeddings. It is trained on large monolingual corpora for 7 languages. Each language's model was trained for approximately 10 epochs. Corpora sizes used in training range from over 270 M tokens in Latvian to almost 2 B tokens in Croatian. About 1 million most common tokens were provided as vocabulary during the training for each language model. The model can also infer OOV words, since the neural network input is on the character level.#SEPThe model is available for download from the CLARIN.SI repository.;Language models;Download;http://hdl.handle.net/11356/1277;;;
LX-DSemVectors;https://hdl.handle.net/21.11129/0000-000B-D38A-B;;Word embeddings;CC-BY;;Portuguese;This model represents tokens as contextual  word embeddings for Portuguese. It was trained on a corpus of 2 billion tokens and achieved state-of-the-art results on multiple lexical semantic tasks.#SEPThe model is available for download from the PORTULAN repository.;Language models;Download;https://hdl.handle.net/21.11129/0000-000B-D38A-B;;;
Slovenian RoBERTa contextual embeddings model: SloBERTa 2.0;http://hdl.handle.net/11356/1397;;word embeddings;CC BY-SA 4.0;;Slovenian;The monolingual Slovene RoBERTa (A Robustly Optimized Bidirectional Encoder Representations from Transformers) model is a state-of-the-art model representing words/tokens as contextually dependent word embeddings, used for various NLP tasks. Word embeddings can be extracted for every word occurrence and then used in training a model for an end task, but typically the whole RoBERTa model is fine-tuned end-to-end.;Language models;Download;;;;
Word Embeddings trained on English Wikipedia;https://spraakbanken.gu.se/en/resources/wikipedia-embeddings;;word embeddings;CC BY 4.0;;Swedish;This is a set of contextual word embeddings.#SEPThe models are available for download from the Swedish Language Bank.;Language models;Download;https://spraakbanken.gu.se/en/resources/wikipedia-embeddings;;;
Word embeddings CLARIN.SI-embed;http://hdl.handle.net/11356/1796;;word embeddings;CC BY-SA 4.0;;Bulgarian, Croatian, Macedonian, Serbian, Slovenian;"This is a set of word embeddings for 5 languages.<ul>
<li>CLARIN.SI-embed.bg contains word embeddings for Bulgarian induced from the MaCoCu-bg web crawl corpus. The embeddings are based on the skip-gram model of fastText trained on 4,120,343,820 tokens of running text for 2,746,640 lowercased surface forms.</li>
<li>CLARIN.SI-embed.hr contains word embeddings induced from a large collection of Croatian texts composed of the Croatian web corpus hrWaC, a 400-million-token-heavy collection of newspaper texts and MaCoCu-hr. The embeddings are based on the skip-gram model of fastText trained on 4,586,769,197 tokens of running text for 3,406,574 lowercased surface forms.</li>
<li>CLARIN.SI-embed.mk contains word embeddings induced from a large collection of Macedonian texts crawled from the .mk top-level domain. The embeddings are based on the skip-gram model of fastText trained on 933,231,582 tokens of running text for 986,670 lowercased surface forms. </li>
<li>CLARIN.SI-embed.sr contains word embeddings induced from the srWaC and MaCoCu-sr web corpora. The embeddings are based on the skip-gram model of fastText trained on 3,434,602,575 tokens of running text for 2,676,036 lowercased surface forms. </li>
<li>CLARIN.SI-embed.sl contains word embeddings induced from a large collection of Slovene texts composed of existing corpora of Slovene, e.g GigaFida, Janes, KAS, slWaC, MaCoCu-sl, etc. The embeddings are based on the skip-gram model of fastText trained on 5,791,405,942 tokens of running text for 3,471,054 lowercased surface forms.</li>
</ul>#SEPThe models are available for download from the CLARIN.SI repository.";Language models;Download (Bulgarian)#SEPDownload (Croatian)#SEPDownload (Macedonian)#SEPDownload (Serbian)#SEPDownload (Slovenian);http://hdl.handle.net/11356/1796#SEPhttp://hdl.handle.net/11356/1790#SEP http://hdl.handle.net/11356/1788#SEPhttp://hdl.handle.net/11356/1789#SEP http://hdl.handle.net/11356/1791;;;
